{
  
    
        "post0": {
            "title": "End to End Deep Learning Project with fastai and FastAPI",
            "content": "Acknowledgment . Before starting on with this tutorial, I would like to mention a few names and resources without which this wouldn&#39;t have been possible. . Firstly, I would like to thank Jeremy Howard, Sylvain Gugger and Rachel Thomas for creating fastai, along with its course and their new book, Deep Learning for Coders with fastai and PyTorch which makes Deep Learning easier and accessible than ever. Then I would like to thank Hamel Husain for creating fastpages, with which this entire blog post was made. It&#39;s such a great tool, and I would encourage everyone thinking of starting a blog to check it out! . And finally, I would like to thank the entire fastai community, for helping me out with my silly doubts on the forums and discord channel, special mention to Zach Mueller! . Introduction . In this tutorial, we&#39;ll look at how easy and fast training a state-of-the-art Convolutional Neural Network on the famous MNIST dataset can be using fastai. Then we&#39;ll use this model to create an API which when provided with an image input, will send back a response to the client. We will then deploy this API using Docker and Heroku to create your very own fully functioning web app. . Since this tutorial is going to be more focused on how to train a Deep Learning model, and deploy it for a web app, I expect the readers to have a basic understanding of how Neural Networks work, along with its underlying concepts such as Backpropagation, Gradient Descent etc. . If you&#39;re unfamiliar with these topics, I highly recommend you to take the fastai course. . Getting Started . Python packages . To get started, we need to install PyTorch, fastai and FastAPI on our system. Ideally, I would recommend everyone to use Anaconda/Miniconda to manage your Python packages and environments. . Assuming you have Anaconda/Miniconda installed, the following lines of code would install the required packages for this tutorial - . conda install pytorch torchvision cudatoolkit=10.2 -c pytorch pip install --upgrade fastai pip install fastapi[all] . Docker and Heroku . Installing is a pretty straightforward process that you can follow in their documentation, and if you&#39;re using Windows or Mac, I highly recommend installing the Docker Desktop. Once you&#39;re done installing it, you&#39;ll also have to sign up, which they ask you during the installation process. . You&#39;ll also need to install the Heroku CLI, which again, you can follow their documentation. Make sure you sign up on Heroku and have an account ready, as you&#39;ll be needing it later in the tutorial. . Development environment . If you&#39;ve followed the fastai course, you would know that the best way to write fastai code is in a IPython notebook environment, as fastai provides many additional features which help your training process much easier and insightful. Hence I would recommend everyone to either install Jupyterlab on their systems, or check out Google Colab, which provides you with a Notebook environment in your browser, along with free GPU support!!. . For the rest of the tutorial, excluding the fastai code, I would recommend any text editor of your choice, but VSCode would be preferred as it provides with many features like type-checking for FastAPI. . Tip: If your system doesn&#8217;t have GPU support, I highly recommend using Google Colab as it will make training your model much faster! . Preparing our Model . Importing libraries . Believe it or not, but we only need a single import statement thing to train our entire model. . from fastai.vision.all import * . Now some of you might say it is a bad practice to import * in a Python project, but that is the beauty of fastai, which keeps in mind all the issues that would come in doing so, and is written in such a way that it is recommended to import it like this! . Getting our data . To make things easy, I chose on purpose to use the MNIST dataset, which is provided by fastai out-of-the-box. We just have to download it, and untar the downloaded file. This can be easily done with the following code. . path = untar_data(URLs.MNIST) . . Note: Depending on your system, this path object will be of either class pathlib.PosixPath or class pathlib.WindowsPath. This is a built-in feature of Python, which I would say is one of the most useful features I&#8217;ve come across while learning fastai and would highly recommend you to check out the pathlib library. . Before we start creating our datasets and dataloaders, we should always check how our data is organized to get an idea of how we&#39;re supposed to label and divide our data. . path.ls() . (#2) [Path(&#39;testing&#39;),Path(&#39;training&#39;)] . (path/&#39;training&#39;).ls() . (#10) [Path(&#39;training/0&#39;),Path(&#39;training/1&#39;),Path(&#39;training/2&#39;),Path(&#39;training/3&#39;),Path(&#39;training/4&#39;),Path(&#39;training/5&#39;),Path(&#39;training/6&#39;),Path(&#39;training/7&#39;),Path(&#39;training/8&#39;),Path(&#39;training/9&#39;)] . (path/&#39;testing&#39;).ls() . (#10) [Path(&#39;testing/0&#39;),Path(&#39;testing/1&#39;),Path(&#39;testing/2&#39;),Path(&#39;testing/3&#39;),Path(&#39;testing/4&#39;),Path(&#39;testing/5&#39;),Path(&#39;testing/6&#39;),Path(&#39;testing/7&#39;),Path(&#39;testing/8&#39;),Path(&#39;testing/9&#39;)] . (path/&#39;training/0&#39;).ls() . (#5923) [Path(&#39;training/0/1.png&#39;),Path(&#39;training/0/1000.png&#39;),Path(&#39;training/0/10005.png&#39;),Path(&#39;training/0/10010.png&#39;),Path(&#39;training/0/10022.png&#39;),Path(&#39;training/0/10025.png&#39;),Path(&#39;training/0/10026.png&#39;),Path(&#39;training/0/10045.png&#39;),Path(&#39;training/0/10069.png&#39;),Path(&#39;training/0/10071.png&#39;)...] . As we can see from the above blocks of code, our data is first divided into training and testing directories, which then includes a directory for each digit, containing images of that particular digit. . Hence, we see that the labels of the images are their parent directory names. Later in this section, we&#39;ll see how easy it is to label your images using fastai! . Creating our DataBlock . Now that we had a look at our data, and know how to go about labelling them. We now have to create, what we call Dataset and Dataloader in PyTorch, which are the two main classes for representing and accessing training or validation data. . Dataset: A collection that returns a tuple of your independent and dependent variable for a single item. | DataLoader: An iterator that provides a stream of mini-batches, where each mini-batch is a tuple of a batch of independent variables and a batch of dependent variables. | . Since a DataLoader builds on top of a Dataset and adds additional functionality to it (collating multiple items into a mini-batch), it’s often easiest to start by creating and testing Datasets, and then look at DataLoaders after that’s working. . For example, . x = list(np.arange(0,26)) y = [chr(ord(&#39;a&#39;) + i) for i in x] dset = list(zip(x,y)) for i in range(3): print(dset[i]) . (0, &#39;a&#39;) (1, &#39;b&#39;) (2, &#39;c&#39;) . Hence, the dset object in the code above, is a valid Dataset in PyTorch as we can index it, and it returns a tuple of the independent and dependent variable, which in this case is integers from 0 to 26 and letters a to z respectively. . Now if you wish to create a DataLoader for this Dataset, you can use the following function provided by PyTorch: . dloader = torch.utils.data.DataLoader(dset, batch_size=4, shuffle=True, drop_last=True) for i in dloader: print(i) . [tensor([10, 22, 2, 12], dtype=torch.int32), (&#39;k&#39;, &#39;w&#39;, &#39;c&#39;, &#39;m&#39;)] [tensor([ 5, 15, 1, 8], dtype=torch.int32), (&#39;f&#39;, &#39;p&#39;, &#39;b&#39;, &#39;i&#39;)] [tensor([21, 18, 9, 3], dtype=torch.int32), (&#39;v&#39;, &#39;s&#39;, &#39;j&#39;, &#39;d&#39;)] [tensor([14, 19, 23, 25], dtype=torch.int32), (&#39;o&#39;, &#39;t&#39;, &#39;x&#39;, &#39;z&#39;)] [tensor([13, 17, 0, 11], dtype=torch.int32), (&#39;n&#39;, &#39;r&#39;, &#39;a&#39;, &#39;l&#39;)] [tensor([ 6, 20, 24, 4], dtype=torch.int32), (&#39;g&#39;, &#39;u&#39;, &#39;y&#39;, &#39;e&#39;)] . As you can see, iterating over the DataLoader, it returns a list with a tensor representing a batch of independent variables, and along with it, a tuple of its corresponding dependent variables. . The parameters of the above function are pretty self-explanatory except maybe drop_last, which allows you to drop the last batch of data if it is not of the given batch_size as in some cases it is important to have all your batches of similar sizes. . On top of these, fastai provides two classes for bringing your training and validation sets together: . Datasets: An object that contains a training Dataset and a validation Dataset | DataLoaders: An object that contains a training DataLoader and a validation DataLoader | . Now that we understand how Dataset and DataLoaders work, thanks to fastai again, we can leave the hassle of splitting our data into training and validation data, and then making the necessary Datasets and DataLoaders to the amazing high-level data-block API. . For our tutorial, to create the required the DataBlock, the following code should do all the required steps, which we&#39;ll understand as we move on! . dblock = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2), get_y=parent_label, item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2, do_flip=False, pad_mode=&#39;zeros&#39;, min_zoom=0.6, max_zoom=1.5) ) . The parameters required to define the DataBlock object are as follows: . blocks: This represents the type of data we wish to represent, hence for this tutorial, our independent variable is an image, and dependent variable will be an integer, representing a category. Hence our problem in hand is a multi-class Classificiation problem. | get_items: This is a function which is run when given a path usually, returns the list of items. In this case, get_image_files is a function which if given a path, recursively finds out all the image files in the directory, and returns their paths as a list. | splitter: This is a function which when given the list of all items, returns two lists, representing the training set and validation set. In this case, we are randomly splitting the images, keeping 20% for validation set. | get_y: This is function, which is mapped to all the items in the list mentioned above, to extract the dependent variable from it, which we call as y usually. parent_label is a function that takes in a path, and outputs the parent directory name. | item_tfms: In PyTorch, we have functions called Transforms, which when given an image, performs certain operations on it. Here, Resize is a transform, which as the name suggest, resizes the image to shape (128,128). | batch_tfms: This is meant for transforms to perform Data Augmentation. These transformations are applied on the batch as a whole, on the GPU, so that the process is fast. Whereas item_tfms is applied on the CPU usually, and to each and every item in the data. . Note: I would love to go on about Data Augmentation in this tutorial, but since we have so much to cover, I believe it is out of scope for this tutorial but I would again highly recommend reading about it or follow the fastai course. . Tip: If you want to know more about a certain function or object, you can run the code doc(DataBlock) for example, to get the documentation on your notebook environment. To have a look at the source code, you can run DataBlock?? in a cell. Now that we have our DataBlock ready, to get our DataLoaders, we simply run the following code: | . dls = dblock.dataloaders(path, bs=64) . Now to access the training set or validation set, we can run dls.train and dls.valid respectively. . It is good practice usually to have a look at our batch of data before moving on, to see if everything works. We can do this very easily with the following piece of code: . dls.train.show_batch(max_n=8, nrows=2) . Yup, that&#39;s it! That&#39;s all you have to do to have a look at your batch of data. . We can see that our images look slightly distorted, and this is what we call as Data Augmentation, which transforms your images to make it harder for the model to identify, but in practice, makes your model more robust to a variety of inputs. It also obviously solves the problem of lack of training data as now we can create multiple training images from what was initially a single given traning example. . Training our model . Now that we have our DataLoaders ready, we&#39;re finally ready to code up our model. But first, we need to decide which model we want to use. . Back in the day, for any type of classification problem, we would look into models like Logistic Regression, Random Forests, Multi-layer Perceptrons, etc. These models would perform great on tabular data, but in this case we&#39;re dealing with image data. To use these models with image data, we had to flatten the image pixels into a single vector, which represented the whole image, and was put into these models. . The issue with this is that, even though models like Multi-layer Perceptrons have given great results in the past, our input is still linear, and hence the model fails to learn certain features of the image which is lost by flattening the pixel values. . Then in the 1990&#39;s, came the first modern use of Convolutional Neural Networks, abbreviated as CNNs, by Yann LeCun et al. in their paper, “Gradient-Based Learning Applied to Document Recognition” where they used this very MNIST dataset! . Now, going into understanding CNNs is a whole another blog post in itself and there are so many of them out there on the Internet. Hence, I wont be covering this topic in this tutorial, but here&#39;s a blog post I love, explaining how CNNs work in very detail! . But to give you a high-level idea of CNNs, they basically run filters, which have certain weights associated with them, over an image to determine the various features of the image. . IMAGE to be added . In this tutorial, we&#39;ll be using a CNN called Resnets, which comes pretrained on the ImageNet dataset. By pretrained, what I mean is that, this model was trained on the mentioned ImageNet dataset, and then the weights of that model were saved, and now used by Deep Learning lovers all around the world! . Note: This method of using a pretrained model is called Transfer Learning, and I would recommend reading about it once you have a good understanding of CNNs. Fastai makes transfer learning with well-known models like Resnets, super easy with their cnn_learner function, which returns a Learner object. To know more about this, feel free to refer the docs. Basically, a Learner object contains the model, along with features such as plotting losses, printing accuracies as we train, etc. So now, we&#39;ll define our cnn_learner. . learn = cnn_learner(dls, resnet18, pretrained=True, metrics=accuracy) . Guess what? You have a state-of-the-art model ready to train on your data, in just a single line of code! . Tip: To have a look at how the model is defined, try running learn.model in a code cell. . Now if you&#39;re familiar with Backpropagation and Gradient Descent, you might know what a Learning Rate is. It is basically a hyper-parameter that lets you decide how fast your model will learn. But you need to be careful deciding its value, as a value too high or too low, will lead to your model failing to learn properly. . To help you decide a good learning rate, the Learner object comes with this function, Learner.lr_find() which basically runs the training process, which inclues Backpropagation, Gradient Descent, etc, but with the learning rate increasing as it trains. This Learning Rate finder method was introduced by Leslie Smith, in her paper, Cyclical Learning Rates for Training Neural Networks. . So let&#39;s go ahead and find a good learning rate to train our model! . learn.lr_find() . SuggestedLRs(lr_min=0.014454397559165954, lr_steep=0.007585775572806597) . As we see here, a graph of Loss against Learning Rate is plotted. Usually it is suggested to use a learning rate where the curve is steepest since that&#39;s where the model seems to be learning the fastest. One would say why not use the learning rate where loss is minimum, but if you look at the curve, the curve becomes increasing right after its minimum point, and hence the once you start training, the model will start getting worse within a few epochs. . Hence, we&#39;ll go ahead with a learning rate of 7e-3 . learn.fine_tune(5, lr=7e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.582411 | 0.171095 | 0.947000 | 03:25 | . epoch train_loss valid_loss accuracy time . 0 | 0.124255 | 0.035682 | 0.988214 | 03:51 | . 1 | 0.094673 | 0.021345 | 0.993357 | 03:53 | . 2 | 0.062400 | 0.021696 | 0.993714 | 03:55 | . 3 | 0.043637 | 0.014921 | 0.995643 | 03:59 | . 4 | 0.041164 | 0.013886 | 0.996214 | 03:58 | . With that, we just trained a resnet on out training data and got an accuracy of pretty much 100% in just around 20 mins! (which could be even lesser if you use Colab or have a better GPU than mine) . One thing I would like to mention is the function fine_tune that we ran for this model. Since we&#39;re using a pretrained model, how Transfer Learning works is that, apart from the final layer of the model, which allows us to classify, the rest of the model is initially freezed. Frozen layers mean they are not trained and hence their weights are not updated. . Hence, how fine_tune works is that, it first runs one epoch with only the final layer not frozen, and then runs the number of epochs specified by you, with the whole model not frozen. . Now all that&#39;s left to do is to save our model, or rather export our model, which we can do with the following line of code: . learn.export(fname=&#39;mnist_resnet18.pkl&#39;) . With that, we just saw how we go about loading our data, creating DataBlocks, DataLoaders and Datasets, loading our model, pretraining it, and then finally exporting it! . To learn more about this whole process, do check out the fastai course! . Now we can move on to creating our API with FastAPI! .",
            "url": "https://sayantank.github.io/datablog/fastai/fastapi/docker/heroku/mnist/cnn/2020/10/08/mnist-fastapi.html",
            "relUrl": "/fastai/fastapi/docker/heroku/mnist/cnn/2020/10/08/mnist-fastapi.html",
            "date": " • Oct 8, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://sayantank.github.io/datablog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://sayantank.github.io/datablog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://sayantank.github.io/datablog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sayantank.github.io/datablog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}